{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2 - Distributed Data Managment"
      ],
      "metadata": {
        "id": "dolkZ2P-XjOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install findspark, pyspark in case it is not installed - if running on Colab.\n",
        "\n",
        "You can copy the whole notebook to your Google account and work on it on Colab via:\n",
        "\n",
        "File -> Save a copy in Drive -> Open the copied notebook"
      ],
      "metadata": {
        "id": "B-4KhSPEzGhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# * When using the Docker workspace do not run this step * \n",
        "IM_RUNNNING_ON_COLAB = True\n",
        "\n",
        "if IM_RUNNNING_ON_COLAB:\n",
        "\n",
        "  !pip install --force-reinstall pyspark==3.2\n",
        "  !pip install findspark\n"
      ],
      "metadata": {
        "id": "Sc0Uq-wviTFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uplaod the data from Moodle, it's a zip file so simply unzip it\n",
        "\n"
      ],
      "metadata": {
        "id": "8-wxCDerykK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/random_data.parquet.zip"
      ],
      "metadata": {
        "id": "romoPyO9zgt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SparkSession is created outside your function"
      ],
      "metadata": {
        "id": "8YFRi4LWAOL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "from time import time\n",
        "\n",
        "def init_spark(app_name: str):\n",
        "  spark = SparkSession.builder.appName(app_name).getOrCreate()\n",
        "  sc = spark.sparkContext\n",
        "  return spark, sc\n",
        "  \n",
        "spark, sc = init_spark('hw2_kmeans')"
      ],
      "metadata": {
        "id": "Zcrds2fTDRU7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load samples points"
      ],
      "metadata": {
        "id": "PkuNgZSMAcYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = spark.read.parquet(\"random_data.parquet\")\n",
        "data_df.show(5)\n",
        "\n",
        "# You can load the small sample for quick testing and reproducing results:\n",
        "\n",
        "sample_df = spark.read.option(\"header\",True) \\\n",
        "                    .option('inferSchema', True)\\\n",
        "                    .csv('sample_data_84.csv')\n",
        "sample_df.show(5)"
      ],
      "metadata": {
        "id": "gspcQQ0qESgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create initials centroids"
      ],
      "metadata": {
        "id": "bX0BGIubAkpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_centroids = \\\n",
        "  spark.createDataFrame([[6.693, 7.782, 5.63],\n",
        "                         [3.744, 4.341, 7.225],\n",
        "                         [9.01, 7.8, 8.03],\n",
        "                         [2.134, 1.59, 1.93]])\n",
        "init_centroids.show()"
      ],
      "metadata": {
        "id": "brc1svO7_unw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Place your kmeans_fit function here\n",
        "#### Don't forget to also add it in a seperate .py file named HW2_WET_[ID1]_[ID1]"
      ],
      "metadata": {
        "id": "D7iiPxz9ArRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All of your imports should go here\n",
        "# You cannot use any premade k-means nor import sklearn...\n",
        "\n",
        "def kmeans_fit(data: pyspark.sql.DataFrame,\n",
        "               init: pyspark.sql.DataFrame,\n",
        "               k: int = 4,\n",
        "               max_iter: int = 10):\n",
        "  # imports\n",
        "  from pyspark.sql import SparkSession\n",
        "  from pyspark.ml.feature import VectorAssembler\n",
        "  import pyspark.sql.functions as F\n",
        "  spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "  def check_convergence(prev_centroids, centroids):\n",
        "    for i in range(len(centroids)):\n",
        "      if list(prev_centroids[i]) != list(centroids[i]):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "  # initialization:\n",
        "  columns = data.columns\n",
        "  converged = False\n",
        "  data_points = data.select(\"*\").persist()\n",
        "  # add a column with number 2 to speed things up late\n",
        "  data_points = data_points.withColumn(\"pow\", F.lit(2))\n",
        "  # create a list of centroids\n",
        "  new_centroids = [list(centroid) for centroid in init.collect()]\n",
        "  # print(centroids)\n",
        "  for iteration in range(max_iter):\n",
        "      if converged:\n",
        "          break\n",
        "      else:\n",
        "        prev_centroids = new_centroids    \n",
        "        distances = []\n",
        "        for i,centroid in enumerate(prev_centroids): \n",
        "            distances.append(f'dist_from_c{i}')  \n",
        "            # for each centroid calculate sigma{(x_j-c_j)^2} by creating (x_j-c_j)^2 column for each j\n",
        "            for j in range(len(centroid)):\n",
        "                data_points = data_points.withColumn(f\"x_{j}_minus_c{j}_pow\", F.expr(f\"{columns[j]} - {centroid[j]}\"))\\\n",
        "                                            .withColumn(f\"x_{j}_minus_c{j}_pow\", F.pow(f\"x_{j}_minus_c{j}_pow\", \"pow\"))\n",
        "            # now sum (x_j-c_j)^2 columns to get euclidian distance from centroid i\n",
        "            # no need for sqrt since its monotonicly increasing and distances are strictly positive\n",
        "            data_points = data_points.withColumn(f'dist_from_c{i}', sum([F.col(f\"x_{k}_minus_c{k}_pow\") for k in range(len(centroid))]))\n",
        "            \n",
        "        # create a column with the assigned centroid for each point\n",
        "        cond = F.expr(\"CASE \" + \" \".join([f\"WHEN {c} = minimum THEN '{i}'\" for i,c in enumerate(distances)]) + \" END\")\n",
        "        data_points = data_points.withColumn('minimum', F.least(*distances)).withColumn(\"centroid_id\", cond)  \n",
        "\n",
        "        # compute new centroids by averaging the points per centroid\n",
        "        new_centroids = data_points.withColumn(\"centroid_id\",F.col(\"centroid_id\").cast(\"int\"))\\\n",
        "                                    .groupBy(\"centroid_id\").avg(*columns).orderBy(\"centroid_id\")\n",
        "        #\n",
        "        new_centroids = [list(centroid[1:]) for centroid in new_centroids.collect()]\n",
        "        # check convergence\n",
        "        if iteration > 1:\n",
        "            converged = check_convergence(prev_centroids, new_centroids)\n",
        "\n",
        "  centroids = spark.createDataFrame(new_centroids)\n",
        "  vecAssembler = VectorAssembler(inputCols=centroids.columns,outputCol=\"centroids\")\n",
        "  return vecAssembler.transform(centroids).select(\"centroids\")"
      ],
      "metadata": {
        "id": "Vk7qwMdG731_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test your function output and run time"
      ],
      "metadata": {
        "id": "jDkdDk4RiYk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time()\n",
        "out = kmeans_fit(data_df, init_centroids)\n",
        "end_time = time()\n",
        "\n",
        "print('Final results:')\n",
        "out.show(truncate=False)\n",
        "print(f'Total runtime: {end_time-start_time:.3f} seconds')"
      ],
      "metadata": {
        "id": "PwaErm_rpGsu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcbe8f63-50f8-4941-b0a8-aebc7a20d1f8"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "+--------------------------------------------------------+\n",
            "|centroids                                               |\n",
            "+--------------------------------------------------------+\n",
            "|[6.500257701999981,6.499862152000027,6.500300249000017] |\n",
            "|[4.500187320000003,4.500350787999989,4.500139439999997] |\n",
            "|[8.499745406999969,8.50008152299997,8.49965887499999]   |\n",
            "|[1.5000080700000005,1.499990830999997,1.500135027500003]|\n",
            "+--------------------------------------------------------+\n",
            "\n",
            "Total runtime: 4.384 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expected results\n",
        "For the given intialization centroids and the sample_df_84 you got on Moodle, the expected results is:\n",
        "\n"
      ],
      "metadata": {
        "id": "opzr_aK7U8xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  +----------------------------------------------------------+\n",
        "#  |centroid                                                  |\n",
        "#  +----------------------------------------------------------+\n",
        "#  |[4.496670602125162,4.495811688311686,4.50180401416766]    |\n",
        "#  |[1.4891561106155216,1.5075798394290807,1.4981257805530763]|\n",
        "#  |[8.501673279603231,8.490239925604461,8.505297582145058]   |\n",
        "#  |[6.496611142694714,6.508118249005111,6.510762933484945]   |\n",
        "#  +----------------------------------------------------------+"
      ],
      "metadata": {
        "id": "QvwifPoAU75N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "jupyter nbconvert --to HTML lab2_318155843.ipynb"
      ],
      "metadata": {
        "id": "EblPPBp0bmDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the algorithm is deterministic, and we expect your results to be the same for at least 3 decimal numbers after the point.\n",
        "\n",
        "\n",
        "The ordering of the centroids in the Dataframe and the title may be different.\n",
        "\n",
        "## Don't forget to run your function on the WHOLE data and show the results in the notebook's PDF and HTML :)\n",
        "## Also don't forget to also add your function in a seperate .py file named HW2_WET_[ID1]_[ID1]"
      ],
      "metadata": {
        "id": "brrgB9YQV2h0"
      }
    }
  ]
}